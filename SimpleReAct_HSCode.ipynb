{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/budianto98/simple-llm-based-scrapper/blob/main/SimpleReAct_HSCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install python-dotenv\n",
        "# ! pip install langchain-community\n",
        "# ! pip install langchain\n",
        "# ! pip install openai\n",
        "# ! pip install langchain-openai\n",
        "# ! pip install faiss-cpu\n",
        "# ! pip install serpapi\n",
        "# ! pip install google-search-results"
      ],
      "metadata": {
        "id": "d3d9T5A9bNLc"
      },
      "id": "d3d9T5A9bNLc",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f8f75193-c6d2-4f0a-924b-f2195ba89861",
      "metadata": {
        "id": "f8f75193-c6d2-4f0a-924b-f2195ba89861"
      },
      "source": [
        "# 0. Loading the environment Variable for chatgpt, serper_api, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "67c2b042-5cc9-4fcf-a1fe-315d8e6de293",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "67c2b042-5cc9-4fcf-a1fe-315d8e6de293"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv,find_dotenv, dotenv_values\n",
        "dd = load_dotenv(find_dotenv())\n",
        "\n",
        "config = dotenv_values(\".env\")\n",
        "# for p in config:\n",
        "#     print(p, config[p])\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7427fb05-699c-4b0d-a964-6e8ea773b6ec",
      "metadata": {
        "id": "7427fb05-699c-4b0d-a964-6e8ea773b6ec"
      },
      "source": [
        "# 1. Agent with *ReAct* Logic\n",
        "\n",
        "There are two tools used by this agent:\n",
        "1. Customize Conversational memory\n",
        "2. GoogleSearch API\n",
        "\n",
        "There is one Agent (1.2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21526414-a11e-4dbf-b442-530910ef0d20",
      "metadata": {
        "id": "21526414-a11e-4dbf-b442-530910ef0d20"
      },
      "source": [
        "## 1.1. Tools\n",
        "\n",
        "*Customized Conversation Memory Tool*\n",
        "\n",
        "It has two input (as we can see from the prompt):\n",
        "1. Chat History {history}\n",
        "2. Human Description {input}\n",
        "\n",
        "and a memory based on HS Code\n",
        "\n",
        "Below is the details:\n",
        "- First we make the prompt template (1.1.1) and create the llm model (1.1.2).\n",
        "- Than we create the conversation modelwith memory of HSCode from Faiss embedding data (1.1.3)\n",
        "- Finally we create Google Search API tool and put them together in a list\n",
        "\n",
        "### 1.1.1. PROMPT template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b8925c9d-4d44-46bb-94dc-94484c65ed8c",
      "metadata": {
        "id": "b8925c9d-4d44-46bb-94dc-94484c65ed8c"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "_DEFAULT_TEMPLATE = \"\"\"\n",
        "Here are 2 examples of the HS code reference:\n",
        "(index: 3011\n",
        "section: XVI\n",
        "hscode: 854141\n",
        "description: Electrical apparatus; photosensitive semiconductor devices, light emitting diodes (LED)\n",
        "level: 6\n",
        "parent: 85414\n",
        "Answer: The HS Code for the above item is 854141. Description is \"Electrical apparatus; photosensitive semiconductor devices, light emitting diodes (LED)\". Index of the HS code 854141 is 3011.)\n",
        "\n",
        "(index: 322\n",
        "section: I\n",
        "hscode: 030543\n",
        "description: Fish; smoked, whether or not cooked before or during smoking, trout (Salmo trutta, Oncorhynchus mykiss/clarki/aguabonita/gilae/apache/chrysogaster), includes fillets, but excludes edible fish offal\n",
        "level: 6\n",
        "parent: 03054\n",
        "Answer: The HS Code for the above item is 030543. Description is \"Fish: smoked, whether or not cooked before or during smoking, trout (Salmo trutta, Oncorhynchus mykiss/clarki/aguabonita/gilae/apache/chrysogaster), includes fillets, but excludes edible fish offal\". Index of the HS code 030543 is 322.)\n",
        "\n",
        "If none of the HS code items related to human's description, treat the human's description as unknown object, and use the following format:\n",
        "Answer: It seems that the object you described is unregistered, please use 999999 as your HS code instead.\n",
        "\n",
        "Below is the chat history:\n",
        "{history}\n",
        "\n",
        "Human's current description: {input}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa61ca8c-9969-4da9-946d-ed6b6516322f",
      "metadata": {
        "id": "fa61ca8c-9969-4da9-946d-ed6b6516322f"
      },
      "source": [
        "### 1.1.2. llm_conv (LLM Model based on Azure OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "32e661d4-0c19-4c65-840b-f2334c096d12",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "32e661d4-0c19-4c65-840b-f2334c096d12"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "\n",
        "\n",
        "llm_conv = AzureChatOpenAI(\n",
        "    temperature=0,\n",
        "     deployment_name=f\"{os.getenv('AZURE_OPENAI_MODEL_3')}\",\n",
        "      openai_api_version=f\"{os.getenv('AZURE_OPENAI_API_VERSION_3')}\",\n",
        "      openai_api_key=f\"{os.getenv('AZURE_OPENAI_API_KEY_3')}\",\n",
        "      azure_endpoint=f\"{os.getenv('AZURE_OPENAI_ENDPOINT_3')}\",\n",
        "    verbose=True\n",
        "    # model_name='systems-mt-evaluation-gpt-4',\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c9292f-7cf8-4ecb-ac98-e6e311d4e86b",
      "metadata": {
        "id": "16c9292f-7cf8-4ecb-ac98-e6e311d4e86b"
      },
      "source": [
        "### 1.1.3. Conversation Chat Tool with HSCode embedding data store in faiss format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8f15fd1e-1f70-4420-b1d3-c02e18f04612",
      "metadata": {
        "id": "8f15fd1e-1f70-4420-b1d3-c02e18f04612"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "\n",
        "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "\n",
        "\n",
        "azure_embeddings = AzureOpenAIEmbeddings(\n",
        "    openai_api_key=f\"{os.environ['OPENAI_API_KEY']}\",\n",
        "    azure_endpoint=f\"{os.environ['OPENAI_API_ENDPOINT']}\",\n",
        "    openai_api_type=f\"{os.environ['OPENAI_API_TYPE']}\",\n",
        "    openai_api_version=f\"{os.environ['OPENAI_API_VERSION']}\",\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "faiss_file_path = \"data/faiss_db/\"\n",
        "faiss_index_name = \"wco_hscode\"\n",
        "\n",
        "def get_retreiver_tool(search_kwargs:int):\n",
        "    faiss_db = FAISS.load_local(faiss_file_path, azure_embeddings, index_name=faiss_index_name, allow_dangerous_deserialization=True)\n",
        "    retriever = faiss_db.as_retriever(search_kwargs=dict(k=search_kwargs))\n",
        "    return retriever\n",
        "\n",
        "def get_faiss_db_supported_memory(retriever):\n",
        "    #VectorStoreRetrieverMemory contains the key of memory\n",
        "    vector_memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
        "    return vector_memory\n",
        "\n",
        "\n",
        "vectorstore_chain = ConversationChain(\n",
        "    llm=llm_conv,\n",
        "    prompt=PROMPT,\n",
        "    # We set a very low max_token_limit for the purposes of testing.\n",
        "    memory=get_faiss_db_supported_memory(get_retreiver_tool(4)),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4893e7af-9b29-46c9-8101-73d7376df65f",
      "metadata": {
        "id": "4893e7af-9b29-46c9-8101-73d7376df65f"
      },
      "source": [
        "### 1.1.4. Create Tools (Google Search API and ConversationChain with HSCode information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c80f0383-8854-43a2-8552-cffdc787976b",
      "metadata": {
        "id": "c80f0383-8854-43a2-8552-cffdc787976b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import load_tools\n",
        "from langchain_community.utilities.serpapi import SerpAPIWrapper\n",
        "\n",
        "\n",
        "# Online search tool\n",
        "general_search = SerpAPIWrapper(search_engine={\"engine\":\"google\", \"google_domain\": \"google.com\", \"gl\":\"hk\"},serpapi_api_key=f\"{os.environ['SERPER_API_KEY']}\")\n",
        "t = load_tools([\"google-serper\"])\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Online Search\",\n",
        "        func=general_search.run,\n",
        "        description=\"useful online searching when receiving unknown product description from user. As sometimes user may just provide a brand or product number.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"HS Code Reference\",\n",
        "        func=vectorstore_chain.run,\n",
        "        description=\"this is the hs code reference, uses human provided information to search for the most relevant hs code information.\",\n",
        "    ),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1278fc-38b4-4cd1-ae8b-25e1127b6ca3",
      "metadata": {
        "id": "ae1278fc-38b4-4cd1-ae8b-25e1127b6ca3"
      },
      "source": [
        "# 1.2. main agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04377840-2866-46cd-bbc2-3454ff6401b9",
      "metadata": {
        "id": "04377840-2866-46cd-bbc2-3454ff6401b9"
      },
      "source": [
        "## Custom template\n",
        "\n",
        "See here sample code: https://github.com/langchain-ai/langchain/discussions/12821\n",
        "\n",
        "or https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb\n",
        "\n",
        "here we can see the ReAct Logic is implemented in the Prompt and Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0c41e10b-f720-426e-bb1b-c8d2b60ad883",
      "metadata": {
        "id": "0c41e10b-f720-426e-bb1b-c8d2b60ad883"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import StringPromptTemplate\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
        "from typing import List, Union\n",
        "import re\n",
        "\n",
        "\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(StringPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        return self.template.format(**kwargs)\n",
        "\n",
        "class CustomOutputParser(AgentOutputParser):\n",
        "\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\n.*?Action Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05dcd2cd-d506-4c52-aad2-4aa294e2765e",
      "metadata": {
        "id": "05dcd2cd-d506-4c52-aad2-4aa294e2765e"
      },
      "source": [
        "### Here are the basic template from langchain hub\n",
        "\n",
        "```\n",
        "Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4bc2dfed-8973-4717-a8b1-c128b19209fd",
      "metadata": {
        "id": "4bc2dfed-8973-4717-a8b1-c128b19209fd"
      },
      "outputs": [],
      "source": [
        "base_prompt = \"\"\"\n",
        "You are an HS code identifier, all your response should only base on hscode reference or facts.\n",
        "You should never lie to human.\n",
        "Observe the potential HScode items and remember the most suitable one's HS code, description and index for later use.\n",
        "Use the HS code tool in order to determine which HS code suits most for the user's input.\n",
        "Think back if the HS code is most suitable for the human description before you give answer.\n",
        "You can always ask user for more information about their product if there are still properties missed out.\n",
        "\n",
        "You have access to the following tools:\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "Question: A human input on product description or answer of the previous question of AI\n",
        "Thought: online search for product detail / check the HS code reference book to gain references\n",
        "Action: the action to take, most likely be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer (Do not provide a HScode if I still have any question to ask human) / I should ask human a question to provide more description to match the most detailed hs code decription\n",
        "Final Answer: the HS code answer that fits most to human's description / I should ask human a follow up question\n",
        "\n",
        "Begin!\n",
        "\n",
        "You should encourage human user to describe more about their product by answering them with question.\n",
        "HS code reference contains id,section,hscode,description,level,parent.\n",
        "Level represent number of digits of HS code.\n",
        "Double check the final answer if the HS code is level 6.\n",
        "Double check the final answer if the ID corrispond to HScode final answer.\n",
        "Double check the information provide by user fulfill all items of description to get the most suitable answer.\n",
        "\n",
        "Use the follwing format for your final answer:\n",
        "'The HS code for your product decriptions: 'Human description history'. Matches with the description in reference book: 'Description of the HS code final answer'. So the the most possible HS code is 'A 6-digits HScode', with the index I referenced to is 'The index of HS code final answer'.\n",
        "\n",
        "Here are the wrong examples for HS code final answer, you should not use the below:\n",
        "The HS code for your product decriptions: Meat and edible offal; of fowls of the species Gallus domesticus not cut in pieces, fresh, chilled, frozen, cuts and offal is 1.\n",
        "(This is wrong because of wrong hscode format and missing index, the correct answer should be 02071)\n",
        "The HS code for your product decriptions: Meat and edible offal; of fowls of the species Gallus domesticus not cut in pieces, fresh, chilled, frozen, cuts and offal is 02071, with index 96.\n",
        "(This is wrong because of wrong index number. AI should always check if the index number matches with the HS code answer.)\n",
        "\n",
        "Chat History:\n",
        "{history}\n",
        "\n",
        "current chat\n",
        "Human: {input}\n",
        "AI:\n",
        "{agent_scratchpad}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1ed0994b-0600-491a-915b-37e40ec05529",
      "metadata": {
        "id": "1ed0994b-0600-491a-915b-37e40ec05529"
      },
      "outputs": [],
      "source": [
        "# from classes import *\n",
        "# from faiss_to_memory import *\n",
        "# print(tools)\n",
        "\n",
        "memory_backed_prompt = CustomPromptTemplate(\n",
        "    template=base_prompt,\n",
        "    tools=tools,\n",
        "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
        "    # This includes the `intermediate_steps` variable because that is needed\n",
        "    # Feel free to add varriables into the base template and the input_varriables\n",
        "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7599304-66bd-487c-ad8b-3658e51bdf79",
      "metadata": {
        "id": "b7599304-66bd-487c-ad8b-3658e51bdf79"
      },
      "source": [
        "## 2.2. Creating main agent and chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "98fc31e5-92a1-4f78-b6aa-744fb3b91b26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98fc31e5-92a1-4f78-b6aa-744fb3b91b26",
        "outputId": "ecedd48a-103c-458b-c158-aadda1fcaee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMSingleActionAgent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "main_llm_model = AzureChatOpenAI(\n",
        "    temperature=0.4,\n",
        "     deployment_name=f\"{os.getenv('AZURE_OPENAI_MODEL_3')}\",\n",
        "      openai_api_version=f\"{os.getenv('AZURE_OPENAI_API_VERSION_3')}\",\n",
        "      openai_api_key=f\"{os.getenv('AZURE_OPENAI_API_KEY_3')}\",\n",
        "      azure_endpoint=f\"{os.getenv('AZURE_OPENAI_ENDPOINT_3')}\"\n",
        ")\n",
        "\n",
        "# LLM chain consisting of the LLM and a prompt\n",
        "llm_chain = LLMChain(llm=main_llm_model, prompt=memory_backed_prompt)\n",
        "\n",
        "# output parser\n",
        "output_parser = CustomOutputParser()\n",
        "\n",
        "#tools name\n",
        "tool_names = [tool.name for tool in tools]\n",
        "\n",
        "\n",
        "from langchain.agents import LLMSingleActionAgent\n",
        "\n",
        "main_agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    # stop=[\"\\nThought:\"],\n",
        "    # stop=[\"\\nQuestion:\"],\n",
        "    allowed_tools=tool_names,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33683bef-74f0-4e4e-995e-ff95cd02df5c",
      "metadata": {
        "id": "33683bef-74f0-4e4e-995e-ff95cd02df5c"
      },
      "source": [
        "# 2. Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d52168cd-5fcb-41e1-a7dc-f1a78bacabd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "d52168cd-5fcb-41e1-a7dc-f1a78bacabd5",
        "outputId": "2f70b50e-b6eb-4721-fb96-fea9ce00f007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, how may I help you today\n",
            "Samsung S8\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "Connection error.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ac5aa99c9377>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt_response\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# recalled_memory = memory_recalling(user_input, memory_with_history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "import time\n",
        "\n",
        "previous_conversation_memory = ConversationBufferWindowMemory(k=10)\n",
        "gpt_response = \"Hi, how may I help you today\"\n",
        "previous_conversation_memory.save_context({\"input\": \"Hi\"}, {\"output\": gpt_response})\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(gpt_response + \"\\n\")\n",
        "\n",
        "        # recalled_memory = memory_recalling(user_input, memory_with_history)\n",
        "        agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "            agent=main_agent,\n",
        "            tools=tools,\n",
        "            verbose=True,\n",
        "            memory=previous_conversation_memory,\n",
        "\n",
        "            #use <handle_parsing_errors=\"Check your output and make sure it conforms!\"> to customize the parsing error if action is not taken.\n",
        "            handle_parsing_errors=True,\n",
        "        )\n",
        "\n",
        "        # gpt_response = agent_executor.run(user_input)\n",
        "        gpt_response = agent_executor.invoke({\"input\": user_input})\n",
        "        # previous_conversation_memory.save_context({\"input\": user_input}, {\"output\": gpt_response})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "        # Wait for a few seconds before restarting the loop\n",
        "        time.sleep(1)\n",
        "\n",
        "        # # Restart the loop\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}